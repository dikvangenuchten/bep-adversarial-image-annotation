% Introduction:
% \begin{itemize}
%     \item Introduce Image Caption task (Metrics are explained in Evaluation)
%     \item Introduce Adversarial attacks/samples
%     \item Introduce attention
%     \item Introduce Research topic (i.e. combination of these things)
% \end{itemize}

The image caption generation task is at the cross-section between Computer Vision (CV) and Natural Language Processing (NLP). It requires the computer to understand a visual scene and describe it into a grammatically correct natural sentence. It can also be seen as a translation task, translating an image into natural language. Practical use cases vary from automated describing of images to visually impaired people \cite{mazzoni_2019} to context based image retrieval. For the model to be successful in these tasks it should be accurate and robust. Show-Attend-and-Tell (S.A.T.) \cite{xu2016show} is an end-to-end deep-learning approach to solve the image captioning task. An example prediction can be seen in figure \ref{fig:sat_example_teddy_bear_clean}

For these models to be useful in real world examples they must be robust against small noise on the input image. However, the problem with machine learning is that models can be very susceptible to noise. As small changes to the input can lead to radically different outcomes. As shown by \citeauthor{goodfellow2015explaining} adding a specific (small) noise layer to an image can alter a correct prediction to a very confident wrong prediction. When the generation of the adversarial examples is not that computational expensive, they can be generated and used during training making the model more robust. It is shown that these adversarial examples then act as regularizers during training. Reducing the chance of overfitting. \citeauthor{Kurakin} expands on generating adversarial examples showing that one can also steer the model towards a specific classification, however this comes at an increased computational cost. An example of an adversarial sample on image captioning can be seen in figure \ref{fig:sat_example_teddy_bear_adversarial}.

\begin{figure*}[h]
    \centering
    \subfloat[a][A group of teddy bears sitting on top of a blanket.]{
        \centering
        \includegraphics[width=0.475\textwidth]{figures/adv_0.00_tedy_bear.jpg}
        \label{fig:sat_example_teddy_bear_clean}
    }
    \vspace{\floatsep}
    \subfloat[b][A close up of a person on a suitcase.]{
        \centering
        \includegraphics[width=0.475\textwidth]{figures/adv_0.20_tedy_bear.jpg} % first figure itself
        % \caption{"A group of teddy bears sitting on top of a blanket."}
        \label{fig:sat_example_teddy_bear_adversarial}
    }
    \caption{Example predictions by Show Attend and Tell on a clean image (left) and an adversarial image (right).}

\end{figure*}

Attention mechanisms, such as introduced by \citeauthor{attention_bahdanau}, have been shown to improve various machine learning tasks, one of which is image captioning. It allows the model to focus on different parts of the image at a time to ensure the whole scene is described. The aforementioned S.A.T. also uses this attention mechanism. Furthermore, it improves the explainability of the model, as for each word it can be shown which parts of the image the model "looked" at. However, if the attention is focused on the wrong parts of the image, the model is blind to possible important parts of the model. Hence, this attention mechanism can also be a possible new attack vector. This paper investigates the susceptibility of adversarial attacks specifically targeted at the attention layer, in an attempt to distract the network making it blind to parts of the image.


% Combining these previous findings, S.A.T. can be used to find adversarial examples for image captioning models. These adversarial images can then be used to either improve current datasets by providing hard samples, or in a more malicious way. The latter being especially true when one can specify the output sentence for which an adversarial sample should be created. Improving the robustness against adversarial samples makes the model less susceptible to small changes in the input. Another point that makes S.A.T. an interesting target, is that it uses explicit attention to generate captions.

% \subsection{Motivation and Related Work}

% \subsubsection*{Adversarial Methods}
% Over the last few years variations of the Fast Gradient Sign Method by \cite{goodfellow2015explaining} have been designed. The Iterative Fast Gradient Method by \citeauthor{Kurakin} applies the Fast Gradient Sign Method multiple times. Which is further improved by using various optimization techniques such as momentum \cite{9237700}, and in the case of Show-and-Fool the well known Adam\cite{kingma2017adam} optimizer for 1000 steps. The method proposed by \citeauthor{EvaluatingRobustness} also includes a distance metric in their adversarial optimization instead of explicitly clipping the values. Although very powerful techniques they are also computationally expensive and due to the computational limitations of this bachelor project, not feasible to apply extensively. Moreover, in the case of purely deceiving S.A.T. it is shown that Iterative Fast Gradient Sign Method already provides significant results.


