\subsection*{Image Captioning}
Various techniques have been used to try and solve the image captioning task. Early methods mainly used hand-designed techniques based on template matching, which made them rigid in the sentences they could generate. One of the first end-to-end deep-learning approaches was Show-and-Tell \cite{showandtell}. It uses a CNN to extract most import features from an image, which then are decoded using an LSTM \cite{lstm} to a sentence, which describes the image.
Show Attend and Tell (S.A.T.) proposed by \citeauthor{xu2016show} is an extension to Show-and-Tell, which adds an attention mechanism before the LSTM decoding. This attention allows the model to focus on specific parts of the image when generating a word. An added benefit is that this attention can be visualized, giving insight into what the model looks at to generate a specific word in the output sentence.

Evaluating image captioning tasks is often done using bilingual evaluation understudy (BLEU) \cite{papineni_roukos_ward_zhu_2001}. The primary reason that it is used is that it is currently the most reported metric in image captioning. It is a form of word n-gram precision between the predicted and human-generated reference sentences. It correlates highly with human ratings of captions \cite{showandtell}. An obvious drawback to this is that a synonym of a word can result in a lower score, even though the sentence still is closely related, or by inserting common words such as 'a', 'the', and 'person' the model can achieve a higher score.

S.A.T. achieves a BLEU score of 0.75\footnote[1]{Calculated using NLTK\cite{nltk} bleu score implementation} out of 1.0 on COCO\cite{lin2015microsoft} datasets, the human annotations reach a score of 0.66\footnote[2]{Calculated by comparing the captions with each other.}. Although the score is not state-of-the-art\cite{DBLP:journals/corr/abs-2107-06912} anymore. This model is chosen because it is small and thus can be run locally, and has publicly available implementations \cite{sgrvinod}.

\subsection*{Adversarial Methods}
In the last few years research in the direction of generating adversarial samples for gradient-based models has been published \cite{goodfellow2015explaining,Kurakin} as well as research showing the usefulness of such adversarial samples\cite{Ilyas2019features} to create more robust datasets. The latter stating: "Adversarial vulnerability is a direct result of our models' sensitivity to well-generalizing features in the data." However, these generalizing features are not robust, as models are optimized to do well in the average case. Inserting adversarial examples in training help regularize these non-robust features\cite{https://doi.org/10.48550/arxiv.1611.01236}.

One of the most influential methods in this field has been proposed by \citeauthor{goodfellow2015explaining}. The Fast Gradient Sign Method (FSGM) (ab)uses the differentiability of machine learning models to find an adversarial example. It is a single-step gradient-based approach to optimize the input image such that it maximizes a certain loss value. Various variations on FSGM have been proposed, such as the Iterative Fast Gradient Method \cite{Kurakin}. Which applies multiple small steps of FSGM. It is further improved by adding various optimization techniques such as momentum \cite{9237700}.

Although FSGM was originally designed for classification tasks, it (and variations) have been successfully adapted to other tasks such as object detection \cite{AdversarialAttacksOnFace,AdversarialFasterRCNN,DBLP:journals/corr/abs-1907-10310}, and most notably for this research on image captioning\cite{Hongge}. \citeauthor{Hongge}'s method Show-and-Fool successfully and consistently can attack Show-and-Tell\cite{showandtell}. They do this by using Adam\cite{kingma2017adam} to optimize the input image for 1000 steps targeting specific keywords. In generating captions that contain those specific keywords they achieve a success rate of 95.8\%, this does come at the cost of taking about 38 seconds to generate a single adversarial sample.

