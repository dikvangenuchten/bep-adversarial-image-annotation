The task of generating image captions is at the intersection of Computer Vision and Natural Language Processing. It requires a visual scene to be understood by the computer and then described in text. To ensure they are reliable in real-world scenarios they should be robust against tampering. This research investigates the robustness of Show Attend and Tell (S.A.T.) \cite{xu2016show}, an end-to-end deep-learning approach using an attention mechanism\cite{attention_bahdanau} to generate a caption for the input image. The robustness is evaluated by generating adversarial disturbances on the input image using a variation of the Fast Gradient Sign Method \cite{goodfellow2015explaining}, which can find the input pixels to which the model is most sensitive in generating the output. Two different attacks will be tried, one directly on the output of the model and one targeting the attention to see if it can be distracted. It is shown that the model can be attacked by both methods. Attacking the attention layer can also be visualized and it is shown that the model only focuses on a single latent pixel.