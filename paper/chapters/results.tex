\subsection*{Adversarial Samples}
Adversarial samples are generated using the iterative version of Fast Gradient Sign method as shown in equation \ref{eq:iterative_method}. With $N=10$ satisfactory results can be achieved, however higher $N$ results in even better results for the same $\epsilon$. Higher epsilons did not have an effect on BLEU score beyond $0.08$ as can be seen in figure \ref{adv_bleu_score}. This is in contrast to the cosine similarity as it does decrease further for the higher $\epsilon$.

\begin{figure*}[h]
    \centering
    \begin{minipage}{0.9\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/adversarial_bleu_score_over_epsilon.png} % first figure itself
        \caption{Average BLEU score for adversarial samples.}
        \label{adv_bleu_score}
    \end{minipage}\hfill
    \begin{minipage}{0.9\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/adversarial_cosine_similarity_heatmap.png} % second figure itself
        \caption{Cosine similarity vs epsilon (Axis is not correct yet, is log scale on x.)}
        \label{adv_cosine_similarity}
    \end{minipage}
\end{figure*}

\begin{figure}[h]
    \centering
    \subfloat[a][$\epsilon$=0.000]{
        \includegraphics[width=0.48\textwidth]{figures/caption_teddy_normal.png}
    }
    \vspace{\floatsep}
    \subfloat[b][$\epsilon=0.020, N=100$]{
        \includegraphics[width=0.48\textwidth]{figures/caption_adv_teddy_bear_0.02.png} % second figure itself
    }
    \caption{Attention visualized for a clean (a) and an adversarial image (b) of a successful attack.}
    \label{adv_example_caption}
\end{figure}

% \begin{figure*}[H]
%     \centering
%     \begin{minipage}{0.45\textwidth}
%         \centering
%         \includegraphics[width=0.9\textwidth]{figures/caption_teddy_normal.png} % first figure itself
%     \end{minipage}\hfill
%     \begin{minipage}{0.45\textwidth}
%         \centering
%         \includegraphics[width=0.9\textwidth]{figures/caption_adv_teddy_bear_0.02.png} % second figure itself
%     \end{minipage}
%     \caption{Clean Image (left), Adversarial Image $\epsilon=0.02, N=10$ (right)}
%     \label{adv_example_caption}
% \end{figure*}

As can be seen in figure \ref{adv_example_caption} the attention of S.A.T, even though not explicitly attacked, is not as focused as on the clean image. This is especially visible in images that are successfully attacked. Images for which the model still is able to generate decent captions, still have a good focus on the main subjects in the image (\ref{adv_example_caption_tennis}).


\begin{figure}[H]
    \centering
    \subfloat[a][$\epsilon$=0.000]{
        \includegraphics[width=0.48\textwidth]{figures/caption_clean_tennis_court.png}
    }
    \vspace{\floatsep}
    \subfloat[b][$\epsilon=0.020, N=100$]{
        \includegraphics[width=0.48\textwidth]{figures/caption_adv_tennis_court_0.02.png} % second figure itself
    }
    \caption{Attention visualized for a clean (a) and an adversarial image (b) of an unsuccessful attack.}
    \label{adv_example_caption_tennis}
\end{figure}

% \begin{figure*}[H]
%     \centering
%     \begin{minipage}{0.45\textwidth}
%         \centering
%         \includegraphics[width=0.9\textwidth]{figures/caption_clean_tennis_court.png} % first figure itself
%     \end{minipage}\hfill
%     \begin{minipage}{0.45\textwidth}
%         \centering
%         \includegraphics[width=0.9\textwidth]{figures/caption_adv_tennis_court_0.02.png} % second figure itself
%     \end{minipage}
%     \caption{Clean Image (left), Adversarial Image $\epsilon=0.02, N=10$ (right)}
%     \label{adv_example_caption_tennis}
% \end{figure*}

\subsection*{Distracting Samples}
To distract the model, adversarial samples are created using the iterative method (EQ. \ref{eq:iterative_method}) and the distraction adversarial loss (EQ. \ref{distraction_loss}). The amount of iterations was experimentally found to be good enough in most cases at 100, in which more would result in better distraction at the cost of longer running times. The adversarial attention targets the left top pixel. On the given dataset the attention is nicely divided with all parts of the image getting close to 0.005 of the average attention per pixel. With an epsilon of 0.04 satisfactory results are achieved. The attention of the model clearly focused on the top left on average as can be seen in figure \ref{average_attention_adv}. The top left pixels clearly get the most attention. With the perturbation at most 0.04 the image is visually identical to the human eye (figure \ref{dist_adv_example_img}).

\begin{figure}[H]
    \centering
    \subfloat[a][Average attention on clean images]{
        \includegraphics[width=0.48\textwidth]{figures/distraction_attention_epsilon_0.png}
        \label{average_attention_clean}
    }
    \vspace{\floatsep}
    \subfloat[b][Average attention on adversarial images with $\epsilon$=0.04 at 100 iterations]{
        \includegraphics[width=0.48\textwidth]{figures/distraction_attention_epsilon_0.04.png} % second figure itself
        \label{average_attention_adv}

    }
    \caption{Average attention visualized for clean (a) and adversarial images (b)}
    \label{average_attention}
\end{figure}


The attention and sentence generation for figure \ref{dist_adv_example_img} are visualized in figure \ref{adv_example_att}. The model is not completely distracted and still attends to other parts of the image, however they are not clearly a single object relating to the word that is generated. During the generations of the last few words the attention is focused almost solely on the top left part.

\begin{figure*}[h]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/distraction_adv_sample_0.png} % first figure itself
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/distraction_adv_sample_0.04.png} % second figure itself
    \end{minipage}
    \caption{Clean Image (left), Adversarial Image $\epsilon=0.04, N=100$ (right)}
    \label{dist_adv_example_img}
\end{figure*}



\begin{figure*}
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/caption_distraction_adv_sample_0.png} % first figure itself
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/caption_distraction_adv_sample_0.04.png} % second figure itself
    \end{minipage}
    \caption{Attention on Clean Image (left) and Adversarial Image $\epsilon=0.04, N=100$ (right)}
    \label{adv_example_att}
\end{figure*}

The BLEU score is less affected by the distracting samples, eventhough more iterations are used. The cosine similarity does go down furhter.

\begin{figure}
    \includegraphics[width=0.9\textwidth]{figures/distraction_bleu_score_over_epsilon.png}
    \caption{BLEU score during distraction over epsilon}
\end{figure}
