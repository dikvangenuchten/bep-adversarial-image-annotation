S.A.T. clearly is susceptible to adversarial samples, and also the attention layer can be influenced. It however does require quite some iterations for it to become noticeable. Also, it is not perfect and the model still gives some attention to the rest of the image.

Due to the higher amount of iterations it is also not clear if these adversarial samples are still as transferable as is shown in the related research. However, as show-and-fool\cite{Hongge} iterates for 1000 steps, and show that those images are still highly transferable. It is likely it will have some degree of transferability. It does require proper research to determine it and see the effects on models that do not use explicit attention.

The aforementioned usability as regularizer also is less beneficial due to the required iterations. As it would hamper the training speed by a factor of ~10-100x. However, it could be possible that the single step approaches are already effective enough. Another possible avenue is if the adversarial samples are transferable, they could be added to the training dataset itself.

The weakness of the BLEU score is clearly visible, as after an epsilon of 0.16 it is not affected a lot, while the cosine similarity still is clearly dropping. The model is able to achieve this due to the bias that is visible in the dataset. Some words such as 'a', 'the', 'man', 'woman' and 'people' are present in a lot of sentences. So if the decoder is not sure what it sees, it falls back to this inherent bias. This is clearly visible in figure \ref{fig:attention_distract_field} where the focus is firmly placed on the left top corner. It predicts words like 'a', and 'woman', which both occur very often in the dataset. Also, the structure of the sentences is still good, although the words do not always make sense. They are mostly grammatically correct.
