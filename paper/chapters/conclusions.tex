Although S.A.T. is more difficult to attack in comparison to classification models, S.A.T. is not immune to adversarial examples. Successful attacks are often visible in the attention layer, showing that the model already is susceptible during the encoding and/or attention step. The most important hyperparameter is the number of iterations of applying I-FSGM.
Both attacks are visible in the attention layer, where the attack on the output makes the model focus more blurry. The explicit attack on the attention works as well, making the attention purely focused on the top-left corner.

Attacking the attention is a harder task, as it requires more iterations to achieve a similar result. It is also not certain that these adversarial samples are transferable, especially those that do not employ attention at all. As it could improve our understanding of the attention mechanism.

Finally, it is not known if the model can be made more robust against the adversaries samples by including adversarial samples in the training set. Either by generating them once and adding them or by generating adversarial samples on the fly during training.
