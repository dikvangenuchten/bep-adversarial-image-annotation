% \subsection{Notation}
% \begin{itemize}
%     \item $X$, clean image in the range [0,1] retrieved from the dataset.
%     \item $X^{adv}$, adversarial image generated from $X$ by applying some perpetration to it. Clipped to be in the range [0,1].
%     \item $Clip_{X,\epsilon}(A)$ is the element-wise clipping of $A$ such that $X -\epsilon \leq A \leq X + \epsilon$ holds element-wise.
% \end{itemize}

\subsection{Dataset}
As clean dataset the well known MSCOCO \cite{lin2015microsoft} dataset will be used. It contains 35 thousand images, of which 30 thousand are part of the train set, and 5 thousand of the testing set. Due to the computational limitations, only the test set is used.

\subsection{Model}
The model used, as already introduced in section \ref{chapter:related_work}, will be Show Attend and Tell. It is an interesting model as it uses attention, which can be visualized, to focus on most important places of the image. It was trained on MSCOCO \cite{lin2015microsoft}. S.A.T. uses a CNN as feature extractor to generate high dimensional latent pixels. These latent pixels are then fed to an attention layer, which in combination with an LSTM produces word tokens. It continues until a stop-token has been generated, or if it has generated 50 words, whichever comes first. In practice, it generally does not create sentences that are longer then 20 words. The attention that is used for each word can be visualized (Figure \ref{fig:caption_elephant_clean}) by mapping the attention on the latent pixels back to the original location on the image. The implementation used, is a publicly available reproduction of S.A.T. in PyTorch \cite{NEURIPS2019_9015} is used. \cite{sgrvinod}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/caption_elephant_normal.png}
    \caption{Visualization of attention. The highlighted parts show the attention that the model used to generate each specific word.}
    \label{fig:caption_elephant_clean}
\end{figure*}

\subsection{Generating Adversarial Samples}
Generating adversarial input images can be done by using the Fast Method (EQ. \ref{eq:fast_method}) proposed by \citeauthor{goodfellow2015explaining}.
\begin{equation}
    X^{adv} = X + \epsilon * sign(\nabla_{x}J(X, y_{true}))
    \label{eq:fast_method}
\end{equation}
$X^{adv}$ is the adversarial sample generated by taking the original image $X$ and perpetrated it with the sign of the gradient of the loss function: $J(X, y_{true})$. Maximizes that loss function. The $\epsilon$ is a hyperparameter which can be tuned to ensure that the adversarial image still is visually the same as the original image. Finally, the adversarial image is clipped to ensure it stays in the within the 0 to 1 input range of the model. As can be seen in Figure \ref{fig:fast_noise_examples} (and more examples in the appendix \ref{appendix:fast_noise_examples}), using this method images up to and including $\epsilon=0.02$ are indistinguishable and up to and including $\epsilon=0.16$ recognizable to humans. The $sign$ method in combination with the epsilon ensures $L_{\infty}(X - X^{adv}) \leq \epsilon$.

\begin{figure}[H]
    \centering
    \subfloat[a][$\epsilon$=0.000]{
        \includegraphics[width=0.42\textwidth]{figures/fast_method_group_of_people/group_of_people_0.000.png}
    }
    \vspace{\floatsep}
    \subfloat[b][$\epsilon$=0.020]{
        \includegraphics[width=0.42\textwidth]{figures/fast_method_group_of_people/group_of_people_0.020.png}
        % \caption*{$\epsilon$=0.020}
    }
    \vspace{\floatsep}
    \subfloat[a][$\epsilon$=0.000]{
        \includegraphics[width=0.42\textwidth]{figures/fast_method_group_of_people/group_of_people_0.000.png}
    }
    \vspace{\floatsep}
    \subfloat[c][$\epsilon$=0.160]{
        \includegraphics[width=0.42\textwidth]{figures/fast_method_group_of_people/group_of_people_0.160.png}
        % \caption*{$\epsilon$=0.160}
    }
    \caption{Clean (left) and Adversarial images (right) for varying epsilon values of $0.020$ and $0.160$. Generated using equation \ref{eq:fast_method}. More values of epsilon can be found in appendix \ref{appendix:fast_noise_examples}.}
    \label{fig:fast_noise_examples}
\end{figure}

In practice applying this a single time is often not enough to successfully attack S.A.T. therefore the iterative method will be used as proposed by \citeauthor{Kurakin}. Which repeatedly applies the Fast Gradient Sign Method for $N$ iterations
\begin{equation}
    X^{adv}_{0}, X^{adv}_{n + 1} = Clip_{X, \epsilon}(X^{adv}_{n} + \alpha * sign(\nabla_{x}J(X^{adv}_{n}, y_{true})))
    \label{eq:iterative_method}
\end{equation}
In which, $\alpha$ is a hyperparameter which naively can be set to $\epsilon / N$. Images generated using this method are usually less visually disturb for the same epsilons. Here an epsilon of $0.040$ is nearly indistinguishable, as can be seen in figure \ref{fig:iterative_noise_examples}

\begin{figure}[h]
    \centering
    \subfloat[a][$\epsilon$=0.000]{
        \includegraphics[width=0.45\textwidth]{figures/fast_method_group_of_people/group_of_people_0.000.png}
    }
    \vspace{\floatsep}
    \subfloat[b][$\epsilon$=0.020]{
        \includegraphics[width=0.45\textwidth]{figures/fast_method_group_of_people/group_of_people_0.020.png}
    }
    \centering
    \subfloat[a][$\epsilon$=0.000]{
        \includegraphics[width=0.45\textwidth]{figures/fast_method_group_of_people/group_of_people_0.000.png}
    }
    \vspace{\floatsep}
    \subfloat[b][$\epsilon$=0.020]{
        \includegraphics[width=0.45\textwidth]{figures/fast_method_group_of_people/group_of_people_0.160.png}
    }
    \caption{Clean (left) and Adversarial images (right) for varying epsilon values of $0.020$ and $0.160$. Generated using equation \ref{eq:iterative_method}. More values of epsilon can be found in appendix \ref{appendix:fast_noise_examples}.}
    \label{fig:iterative_noise_examples}
\end{figure}


\subsection*{Distracting Adversarial Sample}
Distraction is a powerful technique often used by real-world adversaries\footnote[1]{Pickpocketers, street magicians, etc.} against humans. As S.A.T. employs attention to generate sentences, it is possible to try and distract it by creating an adversarial sample that makes the model hyperfocused on only part of the image. During training S.A.T. learns to divide the attention roughly equally over the whole image during the generation of a single caption. It does this by including the loss shown in equation \ref{attention_loss}.
\begin{equation}
    L_{attention} = \sum^{L}_i (1 - \sum^{C}_{t} \alpha_{ti}^2)
    \label{attention_loss}
\end{equation}
\noindent With C equal to the amount of words generated by S.A.T., L equal to the amount of latent pixels, and $\alpha_{ti}$ the attention given to latent pixel $i$ for generating word $t$.

Using categorical cross-entropy we can craft an adversarial example which focuses the attention of S.A.T. to a single latent pixel.
\begin{equation}
    L_{distraction} = CrossEntropy(d, \alpha)
    \label{distraction_loss}
\end{equation}
With $d$, $\alpha \in \rm I\!R^{LxC}$ and $d$ be constructed to focus attention on a specific latent pixel. Combining it with the Iterative Method \ref{eq:iterative_method}, results in equation \ref{eq:iterative_distraction_method}

\begin{equation}
    X^{adv}_{0}, X^{adv}_{n + 1} = Clip_{X, \epsilon}(X^{adv}_{n} + \alpha * sign(\nabla_{x}J(X^{adv}_{n}, \alpha)))
    \label{eq:iterative_distraction_method}
\end{equation}

As can be seen in figure \ref{fig:distract_noise_examples} the images are visually less perturbed even with a higher epsilon. With an image with a perturbation of $\epsilon=0.160$ almost indistinguishable from the clean image. Although $\epsilon=0.640$ is visually distorted it is still very recognizable and would still be described the same by a human.

\begin{figure}[h]
    \centering
    \subfloat[a][$\epsilon=0.000$]{
        \includegraphics[width=0.45\textwidth]{figures/distract_method_samples/0.000/img_6.jpg}
    }
    \vspace{\floatsep}
    \subfloat[b][$\epsilon=0.160, N=100$]{
        \includegraphics[width=0.45\textwidth]{figures/distract_method_samples/0.160/img_6.jpg}
    }
    \vspace{\floatsep}
    \subfloat[c][$\epsilon=0.000$]{
        \includegraphics[width=0.45\textwidth]{figures/distract_method_samples/0.000/img_6.jpg}
    }
    \vspace{\floatsep}
    \subfloat[d][$\epsilon=0.640, N=100$]{
        \includegraphics[width=0.45\textwidth]{figures/distract_method_samples/0.640/img_6.jpg}
    }
    \caption{Clean (left) and Adversarial images (right) for varying epsilon values of $0.160$ and $0.640$. Generated using equation \ref{eq:iterative_distraction_method}. More values of epsilon can be found in appendix \ref{appendix:adversarial_samples_various_epsilon} and \ref{appendix:distracting_samples_various_epsilon}.}
    \label{fig:distract_noise_examples}
\end{figure}

\subsection{Evaluation Metrics}
The accuracy of Image Captioning models is often graded by using BLEU score \cite{papineni_roukos_ward_zhu_2001}. It gives a score between 0 and 1 on how good a certain translation is, by comparing the candidate translation to multiple reference translations. The BLEU score is found to correlate strongly with human judgement, however one weakness is that it cannot detect synonyms. To combat that the cosine similarity between the sentences is calculated. First the sentence is embedded by a Universal Sentence Encoder\cite{DBLP:journals/corr/abs-1803-11175} model, this embedding is then used to calculate the cosine similarity.

To determine if the attention of the model is successfully attacked the average attention over all images will be plotted.