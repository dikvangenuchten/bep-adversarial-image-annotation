\subsection{Notation}
\begin{itemize}
    \item $X$, clean image in the range [0,1] retrieved from the dataset.
    \item $X^{adv}$, adversarial image generated from $X$ by applying some perpetration to it. Clipped to be in the range [0,1].
    \item $Clip_{X,\epsilon}(A)$ is the element-wise clipping of $A$ such that $X -\epsilon \leq A \leq X + \epsilon$ holds element-wise.
\end{itemize}

\subsection{Dataset}
As clean dataset the well known MSCOCO \cite{lin2015microsoft} dataset will be used. It contains 35 thousand images, of which 30 thousand are part of the train set, and 5 thousand of the testing set. Due to the computational limitations, only the test set is used.

\subsection{Model}
The model used, as already introduced in section \ref{chapter:introduction}, will be Show Attend and Tell. It is an interesting model as it uses attention, which can be visualized, to focus on most important places of the image.

\subsection{Generating Adversarial Samples}
Generating adversarial input images can be done by using the Fast Method (EQ. \ref{eq:fast_method}) proposed by \citeauthor{goodfellow2015explaining}.
\begin{equation}
    X^{adv} = X + \epsilon * sign(\nabla_{x}J(X, y_{true}))
    \label{eq:fast_method}
\end{equation}
With $X$ being the input image, $\epsilon$ a hyperparameter determining much the original image can be perpetrated and $J(X, y_{true})$ the loss function which to, in the adversarial case, maximize. Finally, the image is clipped ensuring the vector stays within the 0 to 1 input range of the model. As can be seen in Figure \ref{fig:fast_noise_examples} (and bigger size in the appendix \ref{appendix:fast_noise_examples}), using this method images up to and including $\epsilon=0.02$ are indistinguishable and up to and including $\epsilon=0.16$ recognizable to humans. The sign in combination with the epsilon ensures $L_{\infty}(X - X^{adv}) \leq \epsilon$.

\begin{figure}
    \centering
    \subfloat[a][$\epsilon$=0.000]{
        \includegraphics[height=0.48\textwidth]{figures/fast_method_group_of_people/group_of_people_0.000.png}
    }
    \vspace{\floatsep}
    \subfloat[b][$\epsilon$=0.020]{
        \includegraphics[height=0.48\textwidth]{figures/fast_method_group_of_people/group_of_people_0.020.png}
        % \caption*{$\epsilon$=0.020}
    }
    \vspace{\floatsep}
    \subfloat[c][$\epsilon$=0.000]{
        \includegraphics[height=0.48\textwidth]{figures/fast_method_group_of_people/group_of_people_0.000.png}
    }
    \vspace{\floatsep}
    \subfloat[d][$\epsilon$=0.160]{
        \includegraphics[height=0.48\textwidth]{figures/fast_method_group_of_people/group_of_people_0.160.png}
        % \caption*{$\epsilon$=0.160}
    }
    \caption{Clean (left) and Adversarial images (right) for varying epsilon values of $0.020$ and $0.160$. Generated using equation \ref{eq:fast_method}. More values of epsilon can be found in appendix \ref{appendix:fast_noise_examples}.}
    \label{fig:fast_noise_examples}
\end{figure}

In practice applying this a single time is often not enough to successfully attack S.A.T. therefore the iterative method will be used as proposed by \citeauthor{Kurakin}. Which repeatedly applies the Fast Gradient Sign Method for $N$ iterations
\begin{equation}
    X^{adv}_{0}, X^{adv}_{n + 1} = Clip_{X, \epsilon}(X^{adv}_{n} + \alpha * sign(\nabla_{x}J(X^{adv}_{n}, y_{true})))
    \label{eq:iterative_method}
\end{equation}
In which, $\alpha$ is a hyperparameter which naively can be set to $\epsilon / N$. Images generated using this method are usually less visually disturb for the same epsilons. Here an epsilon of $0.040$ is nearly indistinguishable, as can be seen in figure \ref{fig:iterative_noise_examples}

\begin{figure}
    \centering
    \subfloat[a][$\epsilon$=0.000]{
        \includegraphics[height=0.48\textwidth]{figures/fast_method_group_of_people/group_of_people_0.000.png}
    }
    \vspace{\floatsep}
    \subfloat[b][$\epsilon$=0.020]{
        \includegraphics[height=0.48\textwidth]{figures/fast_method_group_of_people/group_of_people_0.020.png}
    }
    \caption{Clean (left) and Adversarial images (right) for varying epsilon values of $0.020$ and $0.160$. Generated using equation \ref{eq:iterative_method}. More values of epsilon can be found in appendix \ref{appendix:fast_noise_examples}.}
    \label{fig:iterative_noise_examples}
\end{figure}


\subsection*{Distracting Adversarial Sample}
Distraction is a powerful technique often used by adversaries in the real world. As S.A.T. employs attention to generate sentences, it is possible to try and distract it by creating an adversarial sample that makes the model hyperfocused on only part of the image. During training S.A.T. learns to divide the attention roughly equally over the whole image during the generation of a single caption. It does this by including the loss shown in equation \ref{attention_loss}.
\begin{equation}
    L_{attention} = \sum^{L}_i (1 - \sum^{C}_{t} \alpha_{ti}^2)
    \label{attention_loss}
\end{equation}
\noindent With C equal to the amount of words generated by S.A.T., L equal to the amount of latent\footnote{S.A.T. uses a CNN architecture as feature extractor before the attention layer. The output of the CNN is described by the authors as the latent pixel space.} pixels, and $\alpha_{ti}$ the attention given to latent pixel $i$ for generating word $t$.

Using categorical cross-entropy we can craft an adversarial example which focuses the attention of S.A.T. to a single latent pixel.
\begin{equation}
    L_{distraction} = CrossEntropy(d, \alpha)
    \label{distraction_loss}
\end{equation}
With $d$, $\alpha \in \rm I\!R^{LxC}$ and $d$ be constructed to focus attention on a specific latent pixel. Combining it with the Iterative Method \ref{eq:iterative_method}, results in equation \ref{eq:iterative_distraction_method}

\begin{equation}
    X^{adv}_{0}, X^{adv}_{n + 1} = Clip_{X, \epsilon}(X^{adv}_{n} + \alpha * sign(\nabla_{x}J(X^{adv}_{n}, \alpha)))
    \label{eq:iterative_distraction_method}
\end{equation}

As can be seen in figure \ref{fig:distract_noise_examples} the images are visually less perturbed even with a higher epsilon. With an image with a perturbation of $\epsilon=0.160$ almost indistinguishable from the clean image. Although $\epsilon=0.640$ is visually distorted it is still very recognizable and would still be described the same by a human.


\begin{figure}[h]
    \centering
    \subfloat[a][$\epsilon$=0.000]{
        \includegraphics[height=0.48\textwidth]{figures/distract_method_samples/0.000/img_6.jpg}
    }
    \vspace{\floatsep}
    \subfloat[b][$\epsilon$=0.160]{
        \includegraphics[height=0.48\textwidth]{figures/distract_method_samples/0.160/img_6.jpg}
    }
    \vspace{\floatsep}
    \subfloat[c][$\epsilon$=0.000]{
        \includegraphics[height=0.48\textwidth]{figures/distract_method_samples/0.000/img_6.jpg}
    }
    \vspace{\floatsep}
    \subfloat[d][$\epsilon$=0.640]{
        \includegraphics[height=0.48\textwidth]{figures/distract_method_samples/0.640/img_6.jpg}
    }
    \caption{Clean (left) and Adversarial images (right) for varying epsilon values of $0.160$ and $0.640$. Generated using equation \ref{eq:iterative_distraction_method}. More values of epsilon can be found in appendix \ref{appendix:distract_noise_examples}.}
    \label{fig:distract_noise_examples}
\end{figure}

\subsection{Evaluation}
To determine if the model is indeed susceptible to distraction the BLEU-4 score \cite{papineni_roukos_ward_zhu_2001} will be calculated, as it is a widely reported metric within the image captioning task. Because the BLEU score checks for direct word occurrences it does not give a complete view on the success of the adversarial attack, as the model can still give a correct description using synonyms. This would result in a low BLEU score, where in fact the model is still performing correctly. To combat this the cosine similarity of the original and adversarial output will be calculated using universal sentence embedding proposed by \citeauthor{DBLP:journals/corr/abs-1803-11175}. It is a separately learned model that embeds an entire sentence.
In the case of distraction, the average attention the model applies on the dataset is also analyzed.
