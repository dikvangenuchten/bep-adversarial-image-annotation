\subsection{Notation}
\begin{itemize}
    \item $X$, clean image in the range [0,1] retrieved from the dataset.
    \item $X^{adv}$, adversarial image generated from $X$ by applying some perpetration to it. Clipped to be in the range [0,1].
    \item $Clip_{X,\epsilon}(A)$ is the element-wise clipping of $A$ such that $X -\epsilon \leq A \leq X + \epsilon$ holds element-wise.
\end{itemize}

\subsection{Dataset}
As clean dataset the well known MSCOCO \cite{lin2015microsoft} dataset will be used. It contains 35 thousand images, of which 30 thousand are part of the train set, and 5 thousand of the testing set. Due to the computational limitations, only the test set is used.

\subsection{Model}
The model used, as already introduced in section \ref{chapter:introduction}, will be Show Attend and Tell. It is an interesting model as it uses attention, which can be visualized, to focus on most important places of the image.

\subsection{Generating Adversarial Samples}
Generating adversarial input images can be done by using the Fast Method (EQ. \ref{FastMethod}) proposed by \citeauthor{goodfellow2015explaining}.
\begin{equation}
    X^{adv} = X + \epsilon * sign(\nabla_{x}J(X, y_{true}))
    \label{FastMethod}
\end{equation}
With $X$ being the input image, $\epsilon$ a hyperparameter determining much the original image can be perpetrated and $J(X, y_{true})$ the loss function which to, in the adversarial case, maximize. Finally, the image is clipped ensuring the vector stays within the 0 to 1 input range of the model. As can be seen in Figure \ref{epsilon_on_images} (and bigger size in the appendix \ref{big_images_with_noise}), using this method images up to and including $\epsilon=0.04$ are nearly indistinguishable and up to $\epsilon=0.16$ very recognizable to humans. The sign in combination with the epsilon ensures $L_{\infty}(X - X^{adv}) \leq \epsilon$.

\begin{figure*}[h]
    \centering
    \begin{minipage}{0.12\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/group_of_people/group_of_people_0.000.png}
        \caption*{$\epsilon$=0.000}
    \end{minipage}\hfill
    \begin{minipage}{0.12\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/group_of_people/group_of_people_0.005.png}
        \caption*{$\epsilon$=0.005}
    \end{minipage}\hfill
    \begin{minipage}{0.12\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/group_of_people/group_of_people_0.010.png}
        \caption*{$\epsilon$=0.010}
    \end{minipage}\hfill
    \begin{minipage}{0.12\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/group_of_people/group_of_people_0.020.png}
        \caption*{$\epsilon$=0.020}
    \end{minipage}\hfill
    \begin{minipage}{0.12\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/group_of_people/group_of_people_0.040.png}
        \caption*{$\epsilon$=0.040}
    \end{minipage}\hfill
    \begin{minipage}{0.12\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/group_of_people/group_of_people_0.080.png}
        \caption*{$\epsilon$=0.080}
    \end{minipage}\hfill
    \begin{minipage}{0.12\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/group_of_people/group_of_people_0.160.png}
        \caption*{$\epsilon$=0.160}
    \end{minipage}\hfill
    \begin{minipage}{0.12\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/group_of_people/group_of_people_0.320.png}
        \caption*{$\epsilon$=0.320}
    \end{minipage}\hfill
    \caption{Adversarial images for varying values of epsilon.}
    \label{epsilon_on_images}
\end{figure*}

In practice applying this a single time is often not enough to successfully attack S.A.T. therefore the iterative method will be used as proposed by \citeauthor{Kurakin}. Which repeatedly applies the Fast Gradient Sign Method for $N$ iterations
\begin{equation}
    X^{adv}_{0}, X^{adv}_{n + 1} = Clip_{X, \epsilon}(X^{adv}_{n} + \alpha * sign(\nabla_{x}J(X^{adv}_{n}, y_{true})))
    \label{IterativeMethod}
\end{equation}
In which, $\alpha$ is a hyperparameter which naively can be set to $\epsilon / N$. Images generated using this method are usually less visually disturb for the same epsilon.

\subsection*{Distracting Adversarial Sample}
Distraction is a powerful technique often used by adversaries in the real world. As S.A.T. employs attention to generate sentences, it is possible to try and distract it by creating an adversarial sample that makes the model hyperfocused on only part of the image. During training S.A.T. learns to divide the attention roughly equally over the whole image during the generation of a single caption. It does this by including the loss shown in equation \ref{attention_loss}.
\begin{equation}
    L_{attention} = \sum^{L}_i (1 - \sum^{C}_{t} \alpha_{ti}^2)
    \label{attention_loss}
\end{equation}
\noindent With C equal to the amount of words generated by S.A.T., L equal to the amount of latent\footnote{S.A.T. uses a CNN architecture as feature extractor before the attention layer. The output of the CNN is described by the authors as the latent pixel space.} pixels, and $\alpha_{ti}$ the attention given to latent pixel $i$ for generating word $t$.

Using categorical cross-entropy we can craft an adversarial example which focuses the attention of S.A.T. to a single latent pixel.
\begin{equation}
    L_{distraction} = CrossEntropy(d, \alpha)
    \label{distraction_loss}
\end{equation}
With $d$, $\alpha \in \rm I\!R^{LxC}$ and $d$ be constructed to focus attention on a specific latent pixel.

\subsection{Evaluation}
To determine if the model is indeed susceptible to distraction the BLEU-4 score \cite{papineni_roukos_ward_zhu_2001} will be calculated, as it is a widely reported metric within the image captioning task. Because the BLEU score checks for direct word occurrences it does not give a complete view on the success of the adversarial attack, as the model can still give a correct description using synonyms. This would result in a low BLEU score, where in fact the model is still performing correctly. To combat this the cosine similarity of the original and adversarial output will be calculated using universal sentence embedding proposed by \citeauthor{DBLP:journals/corr/abs-1803-11175}. It is a separately learned model that embeds an entire sentence.
In the case of distraction, the average attention the model applies on the dataset is also analyzed.
