\documentclass[12pt]{extarticle}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{apacite}
\usepackage{outlines}

\title{Adversarial Attack on Image Annotation}
\author{H.J.M. van Genuchten, 1297333}
\date{March 2020}

\begin{document}

\maketitle

\section{ Goal}
\subsection{General}
Goal of the BEP is to generate a new benchmark tool for Natural Language Processing (NLP). Preferably one that is adversarial as it is less susceptible to overfitting, and can find weaknesses in current state-of-the-art methods.

\subsection{Specific}
I want to create an adversarial attack benchmark targeted at image annotation \cite{xu2016show,venkatesh}. Inspired by "Adversarial Examples" proposed by \citeauthor{szegedy2014intriguing}, which are images with small perturbations that is able to throw off classification models, but are unnoticeable to humans. \citeauthor{szegedy2014intriguing} also found that these adversarial examples generalize across models. I want to investigate if image annotation networks suffer from the same vulnerabilities. Hence, the following research questions:
\begin{outline}
    \1 Are Image Annotation Networks vulnerable to adversarial examples?
    \2 Can the addition of small (crafted) perturbations significantly affect the output of the network?
    \2 Can the addition of small (crafted) perturbations significantly affect the BLUE\cite{papineni_roukos_ward_zhu_2001} score of the network?
    \1 Is the output annotation controllable?
    \2 Can a perturbation be crafted in such a way that a word gets repeated continuously.
    \2 Can a perturbation be crafted in such a way that it forces a certain annotation?
\end{outline}
The priority will be on the first research question, and if time permits I would like to also answer the second research question.

To further minimize the initial scope, I will focus on a single image annotation model, most likely Show, Attend and Tell (S.A.T.) by \citeauthor{xu2016show} as it is a fully deep learning based approach.

% \subsection{Personal}
% Although not directly related, I have some personal goals that I would like to mention and rationalize:
% \begin{itemize}
%     \item  Test Driven Development (TDD): To ensure that the code that is written is correct, modular and easy to change. Also making sure everything is deterministic and therefore reproducable.
%     \item  A better understanding of how Neural Networks work and train. NNs are usefull but to most still a magic black box that just works. I want to be able to better reason as to why certain things will work and others won't.
%     \item  Mono-repo: Keeping everything related to the project under a single github repository. Including the code, paper and other resources.
% \end{itemize}

\section{Pre Study}
Relevant research has been done in the area of image classification, most notably by \citeauthor{goodfellow2015explaining}, who propose a faster way of finding adversarial examples for image classification. Also, the findings by \citeauthor{venkatesh} show that adversarial examples have cross-model generalization. The result of this project could thus also be used to strengthen current datasets. Furthermore, there has been a lot of research into image annotation. One of the more basic full deep learning image annotation models, S.A.T. \cite{xu2016show}, will be my first focus.

\section{Methodology}
First reproducing some relevant papers as to gain experience with the field and project. Starting with (re)producing an adversarial attack on a MNIST classification model \cite{szegedy2014intriguing} to get familiar with producing adversarial examples. Then applying that on a deep learning image annotation model, like the one proposed by \citeauthor{xu2016show}. After which I will combine the two methods to see if S.A.T. is susceptible to adversarial examples. For training and testing I will make use of the publicly available datasets MS COCO \cite{lin2015microsoft} and Flickr8K \cite{Flickr8k}. To verifying that the adversarial examples are effective I will be mainly looking at BLUE \cite{papineni_roukos_ward_zhu_2001} score as it is widely reported in the image annotation literature.

\subsection{Timeline}
I have set up the following schedule for myself. Bolded deadlines are from the university, the rest is a rough sketch to keep myself on schedule.

\begin{table}[h]
    \begin{tabular}{|l|l|}
        \hline
        \multicolumn{1}{|r|}{Date} & Description                                           \\ \hline
        22 February                & Hand in draft of Project plan                         \\ \hline
        \textbf{01 March}          & \textbf{Hand in Project plan}                         \\ \hline
        08 March                   & Reproduced Adversarial Attack on MNIST classification \\ \hline
        22 March                   & Have a working Image Annotation model                 \\ \hline
        10 April                   & Applied Adversarial Attack on Image Annotation        \\ \hline
        \textbf{17 April}          & \textbf{Hand in Partial thesis}                       \\ \hline
        06 May                     & Targeted Output                                       \\ \hline
        03 June                    & Finalized experimentation                             \\ \hline
        \textbf{19 June}           & \textbf{Hand in Final Thesis}                         \\ \hline
    \end{tabular}
\end{table}


\subsection{Technology}
The code will be written in Python, with PyTorch as Machine-Learning backend.
Versioning will be done with git. Both the paper and code can be found on my personal GitHub repository\footnote{\url{https://github.com/dikvangenuchten/bep-adversarial-image-annotation}}.

\bibliographystyle{apacite}
\bibliography{project_plan}

\end{document}
